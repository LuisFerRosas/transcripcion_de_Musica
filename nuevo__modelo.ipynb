{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nuevo_ modelo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisFerRosas/ia3/blob/nuevo/nuevo__modelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Z1r58emPvZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch import Tensor \n",
        "import math"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXDdelqn6jP"
      },
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,\n",
        "                emb_size: int,\n",
        "                dropout: float,\n",
        "                maxlen: int = 6000):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "      pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "      pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "      pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "      pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "      pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "  def forward(self, token_embedding: Tensor):\n",
        "      return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM1DWqnZm7K3"
      },
      "source": [
        "class TmusicTrasforms(nn.Module):\n",
        "  def __init__(self,num_emotions,n_vocabulario_tgt):\n",
        "    super().__init__() \n",
        "    \n",
        "    ################ TRANSFORMER BLOCK #############################\n",
        "    # maxpool the input feature map/tensor to the transformer \n",
        "    # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
        "    self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "    \n",
        "    self.src_tok_emb = TokenEmbedding(2752*2+40, 512)\n",
        "    self.tgt_tok_emb = TokenEmbedding(n_vocabulario_tgt, 512)\n",
        "    self.positional_encoding = PositionalEncoding(512, dropout=0.3)\n",
        "    \n",
        "    transformer_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
        "        nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "        dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
        "        dropout=0.4, \n",
        "        activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
        "    )\n",
        "    \n",
        "    self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "    \n",
        "    transformer_decoder_layer=nn.TransformerDecoderLayer(\n",
        "        d_model=512,\n",
        "        nhead=4,\n",
        "        dim_feedforward=512,\n",
        "        dropout=0.4,\n",
        "        activation='relu'\n",
        "    )\n",
        "    self.transformer_decoder=nn.TransformerDecoder(transformer_decoder_layer,num_layers=6)\n",
        "\n",
        "    ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock1 = nn.Sequential(\n",
        "        \n",
        "        # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    )\n",
        "    ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock2 = nn.Sequential(\n",
        "      # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    \n",
        "    )\n",
        "\n",
        "    ################# FINAL LINEAR BLOCK ####################\n",
        "    # Linear softmax layer to take final concatenated embedding tensor \n",
        "    #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
        "    # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
        "    # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
        "    # 512*2+40 == 1064 input features --> 8 output emotions \n",
        "    self.fc1_linear = nn.Linear(2752*2+40,num_emotions) \n",
        "    \n",
        "    ### Softmax layer for the 8 output logits from final FC linear layer \n",
        "    self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
        "    \n",
        "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
        "  def forward(self,x,partitura_tok):\n",
        "    \n",
        "    ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
        "    # create final feature embedding from 1st convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
        "    print(\"conv2d_embedding1 : \"+str(conv2d_embedding1.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
        "    print(\"conv2d_embedding1 flatten : \"+str(conv2d_embedding1.shape))\n",
        "    ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
        "    # create final feature embedding from 2nd convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
        "    print(\"conv2d_embedding2 : \"+str(conv2d_embedding2.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
        "    print(\"conv2d_embedding2 flatten : \"+str(conv2d_embedding2.shape))\n",
        "      \n",
        "    ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
        "    # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
        "    x_maxpool = self.transformer_maxpool(x)\n",
        "    print(\"x_maxpool : \"+str(x_maxpool.shape))\n",
        "    # remove channel dim: 1*40*70 --> 40*70\n",
        "    x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "    print(\"x_maxpool_reduced : \"+str(x_maxpool_reduced.shape))\n",
        "    # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
        "    # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
        "    x = x_maxpool_reduced.permute(2,0,1) \n",
        "    print(\"x------>entrada transformer : \"+str(x.shape))\n",
        "    # finally, pass reduced input feature map x into transformer encoder layers\n",
        "    transformer_output = self.transformer_encoder(x)\n",
        "    print(\"salida transformer : \"+str(transformer_output.shape))\n",
        "    \n",
        "    # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
        "    # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
        "    transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
        "    print(\"transformer_embedding  : \"+str(transformer_embedding.shape))\n",
        "  \n",
        "    complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
        "    print(\"complete_embedding  : \"+str(complete_embedding.shape))\n",
        "    complete_embedding=complete_embedding.transpose(1,0)\n",
        "    print(\"complete_embedding222  : \"+str(complete_embedding.shape))\n",
        "    memory = self.src_tok_emb(complete_embedding)\n",
        "    print(\"memory  : \"+str(memory.shape))\n",
        "    # partitura_tok=partitura_tok.transpose(1,0)\n",
        "    # print(\"partitura_tok  : \"+str(partitura_tok.shape))\n",
        "    partitura_tok= self.tgt_tok_emb(partitura_tok)\n",
        "    print(\"partitura_tok2  : \"+str(partitura_tok.shape))\n",
        "    tgt_emb = self.positional_encoding(partitura_tok)\n",
        "    print(\"tgt_emb  : \"+str(tgt_emb.shape))\n",
        "\n",
        "    ouput_decoder=self.transformer_decoder(tgt_emb,memory)\n",
        "    print(\"ouput_decoder  : \"+str(ouput_decoder.shape))\n",
        "    ######### final FC linear layer, need logits for loss #########################\n",
        "    output_logits = self.fc1_linear(complete_embedding)  \n",
        "    print(\"output_logits  : \"+str(output_logits.shape))\n",
        "    ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
        "    output_softmax = self.softmax_out(output_logits)\n",
        "    print(\"output_softmax  : \"+str(output_softmax.shape))\n",
        "    # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
        "    return output_logits, output_softmax     \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcJR6Yq31a1w",
        "outputId": "a7f773e6-6a8b-4472-a4bb-a23f50fcb84c"
      },
      "source": [
        "import torch\n",
        "inputt = torch.randn(2,1, 40,1723, requires_grad=True)\n",
        "partitura = torch.randn(2,133, requires_grad=True)\n",
        "pert = torch.randn(2,5544, requires_grad=True)\n",
        "print(inputt.shape)\n",
        "print(partitura.shape)\n",
        "print(pert.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 40, 1723])\n",
            "torch.Size([2, 133])\n",
            "torch.Size([2, 5544])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43CD3mgLEhD"
      },
      "source": [
        "model=TmusicTrasforms(8,n_vocabulario_tgt=134)\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=model.to(DEVICE)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "Omrl7QgsLsrC",
        "outputId": "0fbd49d4-0b41-49f8-bc79-6ac6e7db3aea"
      },
      "source": [
        "model.train()\n",
        "ouput=model(inputt,partitura)\n",
        "ouput"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_embedding1 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding1 flatten : torch.Size([2, 2752])\n",
            "conv2d_embedding2 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding2 flatten : torch.Size([2, 2752])\n",
            "x_maxpool : torch.Size([2, 1, 40, 430])\n",
            "x_maxpool_reduced : torch.Size([2, 40, 430])\n",
            "x------>entrada transformer : torch.Size([430, 2, 40])\n",
            "salida transformer : torch.Size([430, 2, 40])\n",
            "transformer_embedding  : torch.Size([2, 40])\n",
            "complete_embedding  : torch.Size([2, 5544])\n",
            "complete_embedding222  : torch.Size([5544, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c704893d343e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mouput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartitura\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mouput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0f8fecf86a02>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, partitura_tok)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mcomplete_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplete_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete_embedding222  : \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory  : \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# partitura_tok=partitura_tok.transpose(1,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-87590144c11b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stsn0YOiMr07"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# need device to instantiate model\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# instantiate model for 8 emotions and move to GPU \n",
        "model = TmusicTrasforms(8).to(device)\n",
        "\n",
        "# include input feature map dims in call to summary()\n",
        "summary(model, input_size=[(1,40,1723),(1,100)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nG1FU1ZlJVD",
        "outputId": "51a90c4e-2a1a-4abc-ce88-6f6563195f98"
      },
      "source": [
        "pert.long().dtype"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkaE-eneiis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "31e0be8b-4a5c-4a91-da8d-7207fe4c8fd6"
      },
      "source": [
        "emnb =nn.Embedding(6000,512)\n",
        "\n",
        "salida=emnb(pert.long())\n",
        "salida.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-fab39c4e65e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memnb\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msalida\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msalida\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGll8Xmjxcp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}