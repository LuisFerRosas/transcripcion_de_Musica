{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nuevo_ modelo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisFerRosas/transcripcion_de_Musica/blob/main/nuevo__modelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpnY-7pCEC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50437691-540e-4ecf-d23f-d2114ce52225"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Colab Notebooks/Proyecto_ia3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks/Proyecto_ia3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Z1r58emPvZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch import Tensor \n",
        "import math\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL1nfOKNC7k-",
        "outputId": "f429986e-917f-484e-da52-cd1100e073ad"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " checkpoint\t\t\t    'Final Transformer.ipynb'\n",
            "'Copia de Final Transformer.ipynb'  'nuevo_ modelo.ipynb'\n",
            " datos2\t\t\t\t     transcripcion_de_Musica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7EoQnpDC_yq",
        "outputId": "65f89835-a5cc-4253-f9fe-e95657253413"
      },
      "source": [
        "with open(\"./datos2/vocabulario2.json\", \"r\") as fp:\n",
        "        vocabPart = json.load(fp)\n",
        "\n",
        "len(vocabPart)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "233"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111mOsYKCwwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc58ce7-39a4-4453-8bc5-3cf867b5afa9"
      },
      "source": [
        "!git clone https://github.com/LuisFerRosas/transcripcion_de_Musica.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transcripcion_de_Musica' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DS_rRpxDUOn",
        "outputId": "914d4ec1-e06c-4ec8-eafb-88b13df4922e"
      },
      "source": [
        "cd transcripcion_de_Musica/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks/Proyecto_ia3/transcripcion_de_Musica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNEOJHBFJqu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf89231-379f-467c-f1ad-4ab27e81c07a"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 1)) (57.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 1)) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXDdelqn6jP"
      },
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,\n",
        "                emb_size: int,\n",
        "                dropout: float,\n",
        "                maxlen: int = 6000):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "      pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "      pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "      pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "      pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "      pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "  def forward(self, token_embedding: Tensor):\n",
        "      return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM1DWqnZm7K3"
      },
      "source": [
        "class TmusicTrasforms(nn.Module):\n",
        "  def __init__(self,tgt_vocabulario):\n",
        "    super().__init__() \n",
        "    \n",
        "    ################ TRANSFORMER BLOCK #############################\n",
        "    # maxpool the input feature map/tensor to the transformer \n",
        "    # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
        "    self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "    \n",
        "    \n",
        "    self.tgt_tok_emb = TokenEmbedding(tgt_vocabulario, 512)\n",
        "    self.positional_encoding = PositionalEncoding(512, dropout=0.3)\n",
        "    \n",
        "    transformer_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
        "        nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "        dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
        "        dropout=0.4, \n",
        "        activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
        "    )\n",
        "    \n",
        "    self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "    \n",
        "    transformer_decoder_layer=nn.TransformerDecoderLayer(\n",
        "        d_model=512,\n",
        "        nhead=4,\n",
        "        dim_feedforward=512,\n",
        "        dropout=0.4,\n",
        "        activation='relu'\n",
        "    )\n",
        "    self.transformer_decoder=nn.TransformerDecoder(transformer_decoder_layer,num_layers=6)\n",
        "\n",
        "    ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock1 = nn.Sequential(\n",
        "        \n",
        "        # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            \n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            \n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            \n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            \n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    )\n",
        "    ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock2 = nn.Sequential(\n",
        "      # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    \n",
        "    )\n",
        "\n",
        "    ################# FINAL LINEAR BLOCK ####################\n",
        "    # Linear softmax layer to take final concatenated embedding tensor \n",
        "    #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
        "    # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
        "    # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
        "    # 512*2+40 == 1064 input features --> 8 output emotions \n",
        "    \n",
        "    self.fc2_linear = nn.Linear(512,tgt_vocabulario)\n",
        "    \n",
        "    ### Softmax layer for the 8 output logits from final FC linear layer \n",
        "    self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
        "    \n",
        "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
        "  def forward(self,x,partitura_tok):\n",
        "    \n",
        "    ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
        "    # create final feature embedding from 1st convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
        "    # print(\"conv2d_embedding1 : \"+str(conv2d_embedding1.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
        "    # print(\"conv2d_embedding1 flatten : \"+str(conv2d_embedding1.shape))\n",
        "    ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
        "    # create final feature embedding from 2nd convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
        "    # print(\"conv2d_embedding2 : \"+str(conv2d_embedding2.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
        "    # print(\"conv2d_embedding2 flatten : \"+str(conv2d_embedding2.shape))\n",
        "      \n",
        "    ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
        "    # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70 \n",
        "    x_maxpool = self.transformer_maxpool(x)\n",
        "    # print(\"x_maxpool : \"+str(x_maxpool.shape))\n",
        "    # remove channel dim: 1*40*70 --> 40*70\n",
        "    x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "    # print(\"x_maxpool_reduced : \"+str(x_maxpool_reduced.shape))\n",
        "    # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
        "    # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
        "    x = x_maxpool_reduced.permute(2,0,1) \n",
        "    # print(\"x------>entrada transformer : \"+str(x.shape))\n",
        "    # finally, pass reduced input feature map x into transformer encoder layers\n",
        "    transformer_output = self.transformer_encoder(x)\n",
        "    # print(\"salida transformer : \"+str(transformer_output.shape))\n",
        "    \n",
        "    # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
        "    # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
        "    transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
        "    # print(\"transformer_embedding media : \"+str(transformer_embedding.shape))\n",
        "  \n",
        "    complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
        "    # print(\"complete_embedding  : \"+str(complete_embedding.shape))\n",
        "    cambDim = complete_embedding.unsqueeze(2)\n",
        "    # print(\"cambDim  : \"+str(cambDim.shape))\n",
        "\n",
        "    expandido=cambDim.expand([2,5544,512])\n",
        "    # print(\"expandido  : \"+str(expandido.shape))\n",
        "\n",
        "    memory=expandido.transpose(1,0)\n",
        "    # print(\"memory  : \"+str(memory.shape))\n",
        "      \n",
        "    partitura_tok=partitura_tok.transpose(1,0)\n",
        "    # print(\"partitura_tok  : \"+str(partitura_tok.shape))\n",
        "    partitura_tok= self.tgt_tok_emb(partitura_tok)\n",
        "    # print(\"partitura_tok2  : \"+str(partitura_tok.shape))\n",
        "    tgt_emb = self.positional_encoding(partitura_tok)\n",
        "    # print(\"tgt_emb_positional  : \"+str(tgt_emb.shape))\n",
        "\n",
        "    mask_tgt=generate_square_subsequent_mask(tgt_emb.shape[0])\n",
        "    ouput_decoder=self.transformer_decoder(tgt_emb,memory,mask_tgt)\n",
        "    # print(\"ouput_decoder  : \"+str(ouput_decoder.shape))\n",
        "    linear_decoder=self.fc2_linear(ouput_decoder)\n",
        "    # print(\"linear_decoder  : \"+str(linear_decoder.shape))\n",
        "   \n",
        "   \n",
        "    ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
        "    output_softmax = self.softmax_out(linear_decoder)\n",
        "    # print(\"output_softmax  : \"+str(output_softmax.shape))\n",
        "    # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
        "    return  output_softmax \n",
        "  \n",
        "  \n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMz699GqFy5J"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "def make_train_step(model, criterion, optimizer):\n",
        "    \n",
        "  def train_step(espectro,partituraT,lenPartituras):\n",
        "    model.train() \n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    output_softmax = model(espectro,partituraT)\n",
        "    salida =output_softmax.transpose(0,1)\n",
        "   \n",
        "   \n",
        "    sumAcuraccy=0\n",
        "    for i in range(salida.shape[0]):\n",
        "      predictions=torch.argmax(salida[i],dim=1)\n",
        "      pred=predictions.detach().cpu().numpy()\n",
        "      parti=partituraT[i].detach().cpu().numpy()\n",
        "      acuraccy=precision_score(pred,parti,average='micro')\n",
        "      \n",
        "      # print(\"acuraccy :: \"+str(acuraccy))\n",
        "      \n",
        "      sumAcuraccy+=acuraccy\n",
        "\n",
        "    sumAcuraccy=sumAcuraccy/salida.shape[0]\n",
        "    \n",
        "    output_lengths = torch.full((output_softmax.shape[1],), output_softmax.shape[0], dtype=torch.long)\n",
        "    output_lengths =output_lengths.to(device)\n",
        "    loss = criterion(output_softmax,partituraT,output_lengths,lenPartituras) \n",
        "    \n",
        "    # compute gradients for the optimizer to use \n",
        "    loss.backward()\n",
        "    \n",
        "    # update network parameters based on gradient stored (by calling loss.backward())\n",
        "    optimizer.step()\n",
        "    \n",
        "    # zero out gradients for next pass\n",
        "    # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    return loss.item(),sumAcuraccy\n",
        "  return train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCyh-upyeh7G"
      },
      "source": [
        "def make_validate_fnc(model,criterion):\n",
        "  def validate(espectro,partituraT,lenPartituras):\n",
        "      \n",
        "    # don't want to update any network parameters on validation passes: don't need gradient\n",
        "    # wrap in torch.no_grad to save memory and compute in validation phase: \n",
        "    with torch.no_grad(): \n",
        "        \n",
        "      # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
        "      model.eval()\n",
        "\n",
        "      # get the model's predictions on the validation set\n",
        "      output_softmax = model(espectro,partituraT)\n",
        "      salida =output_softmax.transpose(0,1)\n",
        "      sumAcuraccy=0\n",
        "      for i in range(salida.shape[0]):\n",
        "        predictions=torch.argmax(salida[i],dim=1)\n",
        "        pred=predictions.detach().cpu().numpy()\n",
        "        parti=partituraT[i].detach().cpu().numpy()\n",
        "        acuraccy=precision_score(pred,parti,average='micro')\n",
        "        # print(\"acuraccy :: \"+str(acuraccy))      \n",
        "        sumAcuraccy+=acuraccy\n",
        "\n",
        "    sumAcuraccy=sumAcuraccy/salida.shape[0]\n",
        "    \n",
        "    output_lengths = torch.full((output_softmax.shape[1],), output_softmax.shape[0], dtype=torch.long)\n",
        "    \n",
        "    loss = criterion(output_softmax,partituraT,output_lengths,lenPartituras) \n",
        "        \n",
        "    return loss.item(),sumAcuraccy\n",
        "  return validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2rD88zEgdQK"
      },
      "source": [
        "def make_save_checkpoint(): \n",
        "  def save_checkpoint(optimizer, model, epoch, filename):\n",
        "    checkpoint_dict = {\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'model': model.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "    torch.save(checkpoint_dict, filename)\n",
        "  return save_checkpoint\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "  checkpoint_dict = torch.load(filename)\n",
        "  epoch = checkpoint_dict['epoch']\n",
        "  model.load_state_dict(checkpoint_dict['model'])\n",
        "  if optimizer is not None:\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "  return epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHuNJNJqpzjd"
      },
      "source": [
        "#hiperparametros\n",
        "batch_size=2\n",
        "inicio_EPOCH=0\n",
        "final_EPOCH=100\n",
        "tgt_vocabulario=len(vocabPart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AohCzU_3v6oD"
      },
      "source": [
        "from preprocess import collate_fn_nuevo, obtenerDatos\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import  DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwRVQ_RHh_ox",
        "outputId": "412b5020-4043-450a-e197-ff277da77f36"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# set device to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'{device} selected')\n",
        "\n",
        "# instantiate model and move to GPU for training\n",
        "model=TmusicTrasforms(tgt_vocabulario=tgt_vocabulario)\n",
        "model=model.to(device)\n",
        "print('Numero de parametros entrenable: ',sum(p.numel() for p in model.parameters()) )\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "criterion = nn.CTCLoss()\n",
        "criterion=criterion.to(device)\n",
        "\n",
        "save_checkpoint = make_save_checkpoint()\n",
        "\n",
        "\n",
        "train_step = make_train_step(model, criterion, optimizer=optimizer)\n",
        "\n",
        "\n",
        "validate = make_validate_fnc(model,criterion)\n",
        "\n",
        "\n",
        "\n",
        "def train(num_epochs,model):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  dataset_train =obtenerDatos(pathPartituras='../datos2/partituras',pathVocabulario='../datos2/vocabulario2.json',\n",
        "                          pathArchivoNPY='../datos2/archivosnumpy/train_audio_nombre.npy')\n",
        "    \n",
        "  dataset_valid =obtenerDatos(pathPartituras='../datos2/partituras',pathVocabulario='../datos2/vocabulario2.json',\n",
        "                          pathArchivoNPY='../datos2/archivosnumpy/valid_audio_nombre.npy')\n",
        "  writer = SummaryWriter(\"runs/transformer\")\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=1, collate_fn=collate_fn_nuevo ,shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, collate_fn=collate_fn_nuevo ,shuffle=True)\n",
        "  estep=0\n",
        "  for epoch in range(num_epochs):      \n",
        "    estep+=1\n",
        "    pbar_train = tqdm(dataloader_train)\n",
        "    \n",
        "    loss_train=[]\n",
        "    acuraccy_train=[]\n",
        "    loss_valid=[]\n",
        "    acuraccy_valid=[]\n",
        "    for i, data in enumerate(pbar_train):\n",
        "      partitura_tokenizada,wave_mfcc,len_partitura = data\n",
        "      # print(wave_mfcc.type())\n",
        "      partitura_tokenizada = partitura_tokenizada.to(device)\n",
        "      # wave_mfcc=wave_mfcc.float()\n",
        "      wave_mfcc = wave_mfcc.to(device)\n",
        "     \n",
        "      len_partitura= len_partitura.to(device)\n",
        "      loss,acuraccy= train_step(wave_mfcc,partitura_tokenizada,len_partitura)\n",
        "      loss_train.append(loss)\n",
        "      acuraccy_train.append(acuraccy)\n",
        "      if estep==1:\n",
        "        writer.add_graph(model ,input_to_model=[wave_mfcc,partitura_tokenizada])\n",
        "\n",
        "    for data2 in dataloader_valid:\n",
        "      partitura_tokenizada2,wave_mfcc2,len_partitura2 = data2\n",
        "      partitura_tokenizada2=partitura_tokenizada2.to(device)\n",
        "      # wave_mfcc=wave_mfcc.float()\n",
        "      wave_mfcc2=wave_mfcc2.to(device)\n",
        "      \n",
        "      len_partitura2=len_partitura2.to(device)\n",
        "      loss_val2,acuraccy_val2=validate(wave_mfcc2,partitura_tokenizada2,len_partitura2)\n",
        "      loss_valid.append(loss_val2)\n",
        "      acuraccy_valid.append(acuraccy_val2)\n",
        "\n",
        "    final_loss_train=sum(loss_train)/len(loss_train)\n",
        "    final_acuraccy_train=sum(acuraccy_train)/len(acuraccy_train)\n",
        "    final_loss_valid=sum(loss_valid)/len(loss_valid)\n",
        "    final_acuraccy_valid=sum(acuraccy_valid)/len(acuraccy_valid)\n",
        "    writer.add_scalar(\"loss_train :\",final_loss_train ,epoch)\n",
        "    writer.add_scalar(\"acuraccy_train :\",final_acuraccy_train ,epoch)\n",
        "    writer.add_scalar(\"loss_valid :\",final_loss_valid ,epoch)\n",
        "    writer.add_scalar(\"acuraccy_valid :\",final_acuraccy_valid ,epoch)\n",
        "\n",
        "    pbar_train.set_description(\"Procesando la epoca %d\"%epoch)\n",
        "    print(f'loss_train: {final_loss_train}, loss_valid: {final_loss_valid} , acuraccy_train: {final_acuraccy_train}, acuraccy_valid: {final_acuraccy_valid}')\n",
        "      \n",
        "    # checkpoint_filename = '/content/gdrive/My Drive/DL/models/checkpoints/parallel_all_you_wantFINAL-{:03d}.pkl'.format(epoch)\n",
        "    # save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0 selected\n",
            "Numero de parametros entrenable:  16258793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERVjPio-vor8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_LHhQRGL2Xz",
        "outputId": "1a1aadb3-4eb1-4844-e88d-f63ca99a0868"
      },
      "source": [
        "train(5,model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            " 88%|████████▊ | 265/300 [09:13<01:16,  2.19s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcJR6Yq31a1w"
      },
      "source": [
        "import torch\n",
        "inputt = torch.randn(2,1, 40,1723, requires_grad=True)\n",
        "partitura = torch.randint( 0,132, (2,100))\n",
        "pato = torch.randn(2,5544, requires_grad=True)\n",
        "lenParitura = torch.randint( 30,50, (2,))\n",
        "prueba = torch.randn(100,2,512, requires_grad=True)\n",
        "print(inputt.shape)\n",
        "print(partitura.shape)\n",
        "print(lenParitura)\n",
        "print(prueba.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43CD3mgLEhD"
      },
      "source": [
        "model=TmusicTrasforms(tgt_vocabulario=233)\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=model.to(DEVICE)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcs6SUZG1cg"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = nn.CTCLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Medw2EI8GmDl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNx6zXYmuQCw"
      },
      "source": [
        "train_step=make_train_step(model,criterion,optimizer)\n",
        "loss=train_step(inputt,partitura,lenParitura)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omrl7QgsLsrC"
      },
      "source": [
        "model.train()\n",
        "ouput=model(inputt,partitura)\n",
        "ouput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stsn0YOiMr07"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# need device to instantiate model\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# instantiate model for 8 emotions and move to GPU \n",
        "model = TmusicTrasforms(8).to(device)\n",
        "\n",
        "# include input feature map dims in call to summary()\n",
        "summary(model, input_size=[(1,40,1723),(1,100)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nG1FU1ZlJVD"
      },
      "source": [
        "alto=torch.max(pato)\n",
        "alto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkaE-eneiis"
      },
      "source": [
        "emnb =nn.Embedding(133,512)\n",
        "\n",
        "salida=emnb(partitura.long())\n",
        "salida.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzdmmtabsXku"
      },
      "source": [
        "pato[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gopw3B0Ss8z5"
      },
      "source": [
        "pato.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGll8Xmjxcp"
      },
      "source": [
        "x = pato.unsqueeze(2)\n",
        "print(x.size())\n",
        "\n",
        "x=x.expand([2,5544,512])\n",
        "print(x.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgO5vFlrsgdz"
      },
      "source": [
        "x[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E47vyryr5z-v"
      },
      "source": [
        "ys = torch.ones(1, 1).fill_(1).type(torch.long)\n",
        "ys.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh95UHmSPrwl"
      },
      "source": [
        "ys = torch.cat([ys,torch.ones(1, 1).type_as().fill_(45)], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfL1qVyHJnM3"
      },
      "source": [
        "log_probs = torch.randn(100, 2, 233).log_softmax(2).detach().requires_grad_()\n",
        "print(log_probs.shape)#salida del modelo\n",
        "targets = torch.randint(0, 233, (2, 100), dtype=torch.long)\n",
        "print(targets.shape)#tokens del las partituras\n",
        "input_lengths = torch.full((2,), 50, dtype=torch.long)\n",
        "print(input_lengths)#salida de 100 caracteristicas salida decoder\n",
        "target_lengths = torch.randint(10,30,(2,), dtype=torch.long)\n",
        "print(target_lengths)#tamanio de las partituras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP6KnWfp4mG"
      },
      "source": [
        "log_probs.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY14yO5pKMea"
      },
      "source": [
        "import torch.nn.functional as F  \n",
        "criterion =nn.CTCLoss()\n",
        "loss=criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}